{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks\n",
    "\n",
    "In `retrievall`, \"chunks\" are collections of arbitrary pieces of documents. A chunk can be as large as a whole document or even a full [corpus](01_corpus.ipynb), or as small as a few words or a single token. Chunks are also not *required* to be contiguous.\n",
    "\n",
    "Broadly speaking, the retrieval process is all about 1) divvying documents up into chunks, and 2) selecting which chunks you want to actually use (usually based on some kind of metric or relevancy score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll use a small example corpus, defined below as `corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<retrievall.core.Corpus at 0x105f709a0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from pyarrow.csv import read_csv\n",
    "from retrievall.ocr import corpus_from_tesseract_table\n",
    "\n",
    "csv_str = (\n",
    "    \"level,page_num,block_num,par_num,line_num,word_num,left,top,width,height,conf,text\\n\"\n",
    "    \"1,1,0,0,0,0,0,0,300,400,-1,\\n\"\n",
    "    \"2,1,1,0,0,0,20,20,110,90,-1,\\n\"\n",
    "    \"3,1,1,1,0,0,20,20,180,30,-1,\\n\"\n",
    "    \"4,1,1,1,1,0,20,20,110,10,-1,\\n\"\n",
    "    \"5,1,1,1,1,1,20,20,30,10,96.063751,The\\n\"\n",
    "    \"5,1,1,1,1,2,60,20,50,10,95.965691,(quick)\\n\"\n",
    "    \"4,1,1,1,2,0,20,40,200,10,-1,\\n\"\n",
    "    \"5,1,1,1,2,1,20,40,70,10,95.835831,[brown]\\n\"\n",
    "    \"5,1,1,1,2,2,100,40,30,10,94.899742,fox\\n\"\n",
    "    \"5,1,1,1,2,3,140,40,60,10,96.683357,jumps!\\n\"\n",
    "    \"3,1,1,2,0,0,20,80,90,30,-1,\\n\"\n",
    "    \"4,1,1,2,1,0,20,80,80,10,-1,\\n\"\n",
    "    \"5,1,1,2,1,1,20,80,40,10,96.912064,Over\\n\"\n",
    "    \"5,1,1,2,1,2,40,80,30,10,96.887390,the\\n\"\n",
    "    \"4,1,1,2,2,0,20,100,100,10,-1,\\n\"\n",
    "    \"5,1,1,2,2,1,20,100,60,10,90.893219,<lazy>\\n\"\n",
    "    \"5,1,1,2,2,2,90,100,30,10,96.538940,dog\\n\"\n",
    "    \"1,2,0,0,0,0,0,0,300,400,-1,\\n\"\n",
    "    \"2,2,1,0,0,0,20,20,110,90,-1,\\n\"\n",
    "    \"3,2,1,1,0,0,20,20,180,30,-1,\\n\"\n",
    "    \"4,2,1,1,1,0,20,20,110,10,-1,\\n\"\n",
    "    \"5,2,1,1,1,1,20,20,30,10,96.063751,The\\n\"\n",
    "    \"5,2,1,1,1,2,60,20,50,10,95.965691,~groovy\\n\"\n",
    "    \"4,2,1,1,2,0,20,40,200,10,-1,\\n\"\n",
    "    \"5,2,1,1,2,1,20,40,70,10,95.835831,minute!\\n\"\n",
    "    \"5,2,1,1,2,2,100,40,30,10,94.899742,dog\\n\"\n",
    "    \"5,2,1,1,2,3,140,40,60,10,96.683357,bounds\\n\"\n",
    "    \"3,2,1,2,0,0,20,80,90,30,-1,\\n\"\n",
    "    \"4,2,1,2,1,0,20,80,80,10,-1,\\n\"\n",
    "    \"5,2,1,2,1,1,20,80,40,10,96.912064,UPON\\n\"\n",
    "    \"5,2,1,2,1,2,40,80,30,10,96.887390,the\\n\"\n",
    "    \"4,2,1,2,2,0,20,100,100,10,-1,\\n\"\n",
    "    \"5,2,1,2,2,1,20,100,60,10,90.893219,sleepy\\n\"\n",
    "    \"5,2,1,2,2,2,90,100,30,10,96.538940,fox\\n\"\n",
    ")\n",
    "\n",
    "corpus = corpus_from_tesseract_table(\n",
    "    read_csv(io.BytesIO(csv_str.encode())), document_id=\"123\"\n",
    ")\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Chunk` objects\n",
    "Chunks in retrievall are represented by the `Chunks` object, via tabular data structures. `Chunks` store which atoms are in which chunks, plus metadata or attributes about each chunk.\n",
    "\n",
    "A `Chunks` object represents a collection of chunks, has three attributes (which need to be supplied on instantiation):\n",
    "* `corpus`: The parent `Corpus` of these chunks. This is necessary for accessing atom metadata—the `Chunks` object itself only stores references to atoms, so it doesn't instrisically have any details on thigs like the actual text content of atoms. If a `Chunks` object is added manually to a `Corpus`, this needs to be the same exact object as the parent corpus.\n",
    "* `chunks`: A PyArrow `Table` that stores the IDs of all chunks of this type, along with any other chunk-level metadata. Chunk level metadata defines values that are specific to each chunk (e.g. spatial information, particular textual representations of chunks, metrics and scores, etc.).\n",
    "* `chunk_atoms`: A PyArrow `Table` that outlines which atoms are parts of which chunks. This table simply references atoms and chunks using atom and chunk ID values, and doesn't store any additional information about what are in the chunks or atoms.\n",
    "\n",
    "In documentation, different kinds of chunk-level metadate (i.e. different columns in the `chunks` table) are usually referred to as different \"attributes\" of the chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ChunkExpr`s\n",
    "\n",
    "Creating new chunks is most easily done with a `ChunkExpr` (chunk expression).\n",
    "\n",
    "`ChunkExpr`s are user-definable classes that have a `__call__` function which accepts a `Corpus` as an input and returns a `Chunks` as an output. The usual pattern is to define parameters for the `ChunkExpr` in its `__init__`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from retrievall.core import Corpus, Chunks, ChunkExpr\n",
    "\n",
    "\n",
    "class MyExampleChunker(ChunkExpr):\n",
    "    def __init__(self, coolness: int, size: int):\n",
    "        self.coolness = coolness\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, corpus: Corpus) -> Chunks:\n",
    "        new_chunks = pa.Table.from_pydict(\n",
    "            {\"id\": range(self.size), \"coolness\": [self.coolness] * self.size}\n",
    "        )\n",
    "\n",
    "        # Which atoms are in which chunks\n",
    "        # (this example just has 1-token chunks)\n",
    "        chunk_atoms = pa.Table.from_pydict(\n",
    "            {\n",
    "                \"chunk\": new_chunks[\"id\"],  # chunk IDs\n",
    "                \"coolness\": corpus.atoms[\"id\"][: self.size],  # atom IDs\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return Chunks(corpus=corpus, chunks=new_chunks, chunk_atoms=chunk_atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining your own `ChunkExpr`s, remember that each chunk needs a unique chunk ID, which you'll have to generate manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chunk expression can be used to create chunks via `Corpus.chunk()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<retrievall.core.Chunks at 0x11d08f310>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.chunk(MyExampleChunker(coolness=10, size=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or standalone by instantiating it and then calling it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<retrievall.core.Chunks at 0x11d08f3a0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate instantation and calling:\n",
    "chunker = MyExampleChunker(coolness=10, size=4)\n",
    "chunks = chunker(corpus)\n",
    "\n",
    "# One-liner\n",
    "chunks = MyExampleChunker(coolness=10, size=4)(corpus)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing chunks\n",
    "\n",
    "Chunks are most easily accessed via `Corpus.chunk()`, to get either pre-existing persistent chunks or to generate ephemeral chunks. See the [Corpus chunks](01_corpus.ipynb#Corpus-chunks) documentation for more details on interfacing with chunks via their parent corpus.\n",
    "\n",
    "You can also create chunks by instantiating them manually or [using a `ChunkExpr`](#chunkexprs). \n",
    "\n",
    "Interacting with chunks is the same whether you use the `Corpus.chunk()` method or manually use a `ChunkExpr` or make a `Chunks` object yourself by hand—each approach gives a `Chunks` object to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving from chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have some chunks, you can start retrieving the most relevant ones!\n",
    "\n",
    "`Chunks` objects have three useful methods for enriching chunks with additional metadata and retrieving chunks based on that metadata:\n",
    "* `Chunks.enrich` adds new columns (or \"attributes\") to the internal `chunks` table, which is helfpul for adding additional metadata to determine relevance.\n",
    "* `Chunks.filter` returns a new `Chunks` object with certain chunks filtered out. This is the act of retrieval itself.\n",
    "* `Chunks.select` materializes the loose collections of atoms and metadata contained in a `Chunks` object into a single, concrete PyArrow `Table`.\n",
    "\n",
    "Here's a fairly minimal example retrieval process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "text: large_string\n",
       "----\n",
       "text: [[\"The (quick)\",\"[brown] fox jumps!\"]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from retrievall.exprs import SimpleStringify\n",
    "from retrievall.filters import Threshold\n",
    "\n",
    "# Retrieve the text of the first 2 lines (a built-in chunk in this corpus)\n",
    "# based on their `ordinal` value (a built-in attribute for line chunks)\n",
    "(\n",
    "    corpus.chunk(\"line\")\n",
    "    .filter(Threshold(\"ordinal\", \"<=\", 2))\n",
    "    .select(text=SimpleStringify())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich\n",
    "The `Chunks.enrich` method is somewhat analagous to Polars'/PySpark's `with_columns`, or Ibis' `mutate` function: it adds new metadata/attributes to the chunk collection, and passes them on alongside all the previously existing attributes.\n",
    "\n",
    "`enrich` accepts multiple `AttrExpr` keyword arguments (and *no* positional arguments!). The name provided for the argument becomes the name of the column in the `chunks` table, which can then be referenced in future operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paragraph', 'left', 'top', 'width', 'height', 'ordinal', 'id']\n",
      "['paragraph', 'left', 'top', 'width', 'height', 'ordinal', 'id', 'my_attr']\n"
     ]
    }
   ],
   "source": [
    "from retrievall.exprs import SimpleStringify\n",
    "\n",
    "names = corpus.chunk(\"line\").chunks.column_names\n",
    "\n",
    "print(names)\n",
    "\n",
    "names = corpus.chunk(\"line\").enrich(my_attr=SimpleStringify()).chunks.column_names\n",
    "\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "The `Chunks.filter` method, like most DataFrame libraries' `filter` functions, removes unwanted chunks, leaving just the ones that are queried for. See the [filters notebook](04_filters.ipynb) for more in-depth info about filters.\n",
    "\n",
    "`filter` accepts multiple `ChunkFilter` *positional* arguments; if multiple filters are provided, they're applied in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from retrievall.filters import TopK\n",
    "\n",
    "chunks = corpus.chunk(\"line\")\n",
    "print(len(chunks))\n",
    "\n",
    "# Filter\n",
    "chunks = corpus.chunk(\"line\").filter(TopK(\"ordinal\", 3))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select\n",
    "\n",
    "`Chunks.select` takes the loose collection of associated data that makes up a `Chunks` object and puts it together into a much more usable DataFrame/table. This lets you control 1) how the contents of your chunks are manifested for any kind of downstream use and 2) *which* attributes and metadata you actually want to use.\n",
    "\n",
    "Similar to [Polars' `select` function](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.select.html) `Chunks.select` allows you to leverage chunks attribute/column names, expressions, and positional/keyword arguments to give you flexibility over what data you select:\n",
    "* Existing chunk attributes can be selected by passing the name of the attribute as a string (e.g. `\"ordinal\"`, or `\"custom_score\"` if you've created an attribute with that name via `Chunks.enrich`)\n",
    "* Attributes can be created on-the-fly by passing an `AttrExpr` as a keyword argument. Like `Chunks.enrich`, the argument name provided will become the name of the column in the table that is returned.\n",
    "* Any attribute not selected will not be returned.\n",
    "\n",
    "`select` returns a PyArrow `Table`, which can easily be converted to any DataFrame library, or to native Python data structures like lists-of-dicts or dicts-of-lists, or serialized to Arrow or Parquet or CSV, or just used as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "ordinal: uint32\n",
       "fancy_text_col: large_string\n",
       "----\n",
       "ordinal: [[1,2,3,4,5,6,7,8]]\n",
       "fancy_text_col: [[\"The (quick)\",\"[brown] fox jumps!\",\"Over the\",\"<lazy> dog\",\"The ~groovy\",\"minute! dog bounds\",\"UPON the\",\"sleepy fox\"]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from retrievall.exprs import SimpleStringify\n",
    "\n",
    "corpus.chunk(\"line\").select(\"ordinal\", fancy_text_col=SimpleStringify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "keep_output": true,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
