{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API workshopping\n",
    "Let's figure out a better API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing component inputs/outputs\n",
    "* Corpus: Nothin'. That's the main thing we're workign with/passing around.\n",
    "* Chunker: settings -> chunks (which may or may not have any additional attributes? And may or may not need to be persistent)\n",
    "    * (If we have lists of things, \"atoms\" may also be able to be considered a \"value for each chunk\")\n",
    "* Scorer: chunks -> a value for each chunk\n",
    "    * We also maybe want a chunk/atom *modifier* (like, something that changes which atoms are in the chunk) and also something that adds/removes chunks to an existing chunk set.\n",
    "* Filter: chunks -> chunks (but less of them). We actually use a \"filter\" for the \n",
    "* Materializer: chunks -> One thing per chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General notes\n",
    "* **Ownership.** Part of the difficulty is that a chunk does't \"own\" its atoms. It's all associations! We usually want to use the contents/attributes of the atoms (e.g. `text` or `ordinal`) when doing anything useful, but we also can't do that directly from the chunk itself because the chunk just knows which atoms it's referring to, not their other attributes (which could be something complicated, up to and including per-atom embeddings?). It seems like it would be nice to access atom attributes directly, but they aren't actually immediately available and requrie some cross-referencing.\n",
    "* **Objects/memory/pointers.** If it's all referential—couldn't we just cover this all with *actual* data/objects? Like rather than referencing IDs everywhere, could we have actual object or pointer? I mean, the fact that this stuff lives in memory means it *already* has a unique address, right? Can't all this references-of-references be handled by using in-memory objects?\n",
    "* **Normal forms.** If we've got custom attributes all the time, maybe 6th normal form *is* helpful? Like, if we register every single attribute separately, maybe that makes it easier to have an interface to access them?\n",
    "* **Emphasizing materialization**. Materializing *does* result in a single value per chunk. At least, allegedly. So maybe we just need to ensure that any pipeline has a materialization step? Like, all the processes lead up to providing something to materialize?\n",
    "    * Sticking with that idea for a sec, are there any cases where we *don't* want to materialize?\n",
    "        * With a chunker, we're either persisting things (so someone can use them later) or we're actively working towards materialization. And we materialize chunks.\n",
    "        * A \"scorer\" is adding metadata to a chunk. Usually as part of a materialization process. If it isn't then it also doesn't need to be cleaned up, it can just exist as tables. This might be better as an \"annotate\" function or something?\n",
    "        * Anyways...\n",
    "* **Atom select statement.** There's something kicking around in here about not just a chunk select, but an atom select. Like, maybe for defining (or modifying) chunks, we need a separate layer of select statements to pick out which atoms we want? So we have an atom select clause and also a chunk select clause?\n",
    "* **Declarative group by operation.** Is this all some kind of crazy, declarative `group_by`?\n",
    "    * In this case, \"persisted\" chunks are just things that we have handy groups pre-defined for. A sorta trivial group by operation.\n",
    "    * Can we filter groups from a GroupBy? (That's a `HAVING` clause, pretty much, but I'm not sure what the options are via PyArrow/Polars/whatever.)\n",
    "    * How would something like the `ChunkExpander` fit in to this? You can't modify what things belong in a group that way at that stage, can you?\n",
    "* **Potential delineations.** There are a few ways we could conceptually slice what we're working on. We just have to figure out the best combination.\n",
    "    * **Chunks & atoms.** Everything is a chunk or an atom. Chunks are made of atoms, and the end goal of what we're trying to do is to grab the right chunks\n",
    "        * **Are chunks immutable?** Once you have a chunk, maybe that's it—you can't do anything else with it. So if you want to expand a chunk (for example), what you would actually have is an `Expander` that takes *existing* chunks (either persisted or ephemeral) and uses them as an input to generate completely new chunks. So an \"expander\" is just a chunker that uses existing chunks as an input.\n",
    "        * **And what is a `Chunks` object?** Based on the above, a `Chunks` object contains both the chunk attributes and the chunk relations. Plus a reference to the parent corpus, probably. So that's actually not too bad, it's just three elements. (We probably want to change the column names of the internal tables, so that there's just one source of truth. Or maybe not even have a name there? Maybe the chunk name exists externally, not internally [i.e. it's just an alias or something?]).\n",
    "        * **Flexibility downsides?** This *does* make it a bit trickier to represent relationships between different kinds of chunks, if people are interested in that. But that also feels like a pret\n",
    "    * **Objects & relationships.**. Everything is an object or a relationship. Atoms and chunks are both just objects (which can have attributes). Which atoms are associated with a chunk are defined by relationships. The primary output of a retrievall process is to operate on a `chunk-atom` relationship, maybe? This paradigm is *really* general, but maybe not the best suited to a situation where we're largely doign the same things over and over... For example, it would let us more easily reflect relationships between multiple kinds of \"chunk\" (which is not a super common thing for our use-cases?), but also seems like it would make it more difficult to actually get a chunk consistently (because then you need to get the atoms, etc). This feels like it's kind of leaning towards a graph or hypergraph sorta thing.\n",
    "* **Leading options.** It feels like there are two leading options right now:\n",
    "    * **Embrace databases.** This would lean towards storing everything in DuckDB and basically writing helper wrapper functions around everything. That would be performant, and maybe elucidate some ideas on how things should work, but may also hurt flexibility? Kinda depends on what the DuckDB interface allows for. It'll take some exploration. The thesis here is that databases are established and flexible technology, and it would be wise to build on them rather than reinventing the wheel.\n",
    "    * **Custom representation.** This leans towards breaking things down even further in a custom way: more nested data, references of references, etc. Thhe thesis for this one is that databses are great and all, but they *aren't* built for handling the type of thing we're trying to do, and we need something more suited to our specific use cases.\n",
    "    * **Does it matter either way?** I'm curious if the backend stuff could be independent of the API. Maybe both are valid, and having a great interface could let either option work well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How DuckDB went\n",
    "* One of the things *I* value is not having to put names on stuff/using variables. (Which maybe isn't super important in the grand scheme of things, but feels like something that helps with generalizability.) With DuckDB, using variable table names feels really difficult—it takes some work to avoid \"officially\" registering a table with a super simple string name. \n",
    "* Managing multiple tables for a single concept is indeed quite tricky. Having `document` and `document-atom` and `atom` all be accessed by having to know the string names and how they tie together is cumbersome, especially when *actually using* the data (e.g. materializing it, scoring it) almost always requires combining multiple data sources.\n",
    "* With that in mind, the whole relational thing really is being put to the test here. While splitting normalizing things like we have is great for data consistency and stuff, having to then bring the data back together all the time is challenging. It may indeed be better to give up on that and start using de-normalized data, if we're going to be joining all the time anyways. (But on the other hand, if we're going to be \"joining al lthe time anyways\", we need to be super careful about *what* we're joining, and who owns what data, and how we access it, etc. For example, we wouldn't want to de-normalize atom attributes by default, because then we may miss some if we add attributes or we may end up duplicating tons and tons of info.)\n",
    "\n",
    "**ColBERT mockup example??** That may be a good test case for actually using something complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "To flesh this out more, I'm gonna throw some examples out here.\n",
    "\n",
    "Let's say we've got tokenized documents (like we do with an `OCRCorpus`), and we're trying to **retrieve the most positive haikus**. What would this process look like?\n",
    "\n",
    "Here are a few potential APIs:\n",
    "### Pure SQL\n",
    "Since `retrievall` is *pretty much* just querying stuff from a database, we can pseudocode some SQL for haiku-querying.\n",
    "\n",
    "First, we need to find where the Haikus are. Since this a bit complex, and maybe we want to re-use the haikus we found in the corpus for something else, we'll persist the chunks to the corpus. Ideally, any API we end up going with should allow us to both persist chunk for re-use *and* just use them ephemerally for one-off tasks.\n",
    "```sql\n",
    "CREATE TABLE haiku_atom AS\n",
    "    SELECT 42 AS i, 84 AS j;\n",
    "\n",
    "SELECT\n",
    "    \n",
    "FROM corpus\n",
    "GROUP BY\n",
    "\n",
    "HAVING\n",
    "```\n",
    "If we wanted to pre-compute haiku chunks, rather than \n",
    "\n",
    "### SQL-like\n",
    "Despite, sharing a lot in common with regular database querying operations, `retrievall` has a much greater focus on grouping atoms and aggregating them. So what we might want instead is something *like* SQL, but with less boilerplate for doing the things we're trying to do.\n",
    "\n",
    "```sql\n",
    "\n",
    "```\n",
    "\n",
    "### Expression-y (Polars/Spark-like)\n",
    "\n",
    "### Other\n",
    "\n",
    "```python\n",
    "corpus.chunk(\"existing_chunk\").filter(tfidf)\n",
    "\n",
    "# Another approach\n",
    "corpus.chunk(fixedwindow(size=100, offset))\n",
    "```\n",
    "\n",
    "Another option: A `Chunks` object contains chunk attributes and also the references to which atoms the chunk contains (A \"view\" of them, as it were). Regardless of how we actually store the info about the atoms in the chunk, accessing the atoms themselves can be done by:\n",
    "```python\n",
    "my_chunks = Chunks() # Whatever does here to make chunks\n",
    "my_chunks.atoms() # Access a groupy_by of all the atom stuff, perhaps? Somehow, this should get us access to the contents of the chunks.\n",
    "```\n",
    "\n",
    "Gotta look at some other approaches, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_csv = (\n",
    "    \"level,page_num,block_num,par_num,line_num,word_num,left,top,width,height,conf,text\\n\"\n",
    "    \"1,1,0,0,0,0,0,0,300,400,-1,\\n\"\n",
    "    \"2,1,1,0,0,0,20,20,110,90,-1,\\n\"\n",
    "    \"3,1,1,1,0,0,20,20,180,30,-1,\\n\"\n",
    "    \"4,1,1,1,1,0,20,20,110,10,-1,\\n\"\n",
    "    \"5,1,1,1,1,1,20,20,30,10,96.063751,The\\n\"\n",
    "    \"5,1,1,1,1,2,60,20,50,10,95.965691,(quick)\\n\"\n",
    "    \"4,1,1,1,2,0,20,40,200,10,-1,\\n\"\n",
    "    \"5,1,1,1,2,1,20,40,70,10,95.835831,[brown]\\n\"\n",
    "    \"5,1,1,1,2,2,100,40,30,10,94.899742,fox\\n\"\n",
    "    \"5,1,1,1,2,3,140,40,60,10,96.683357,jumps!\\n\"\n",
    "    \"3,1,1,2,0,0,20,80,90,30,-1,\\n\"\n",
    "    \"4,1,1,2,1,0,20,80,80,10,-1,\\n\"\n",
    "    \"5,1,1,2,1,1,20,80,40,10,96.912064,Over\\n\"\n",
    "    \"5,1,1,2,1,2,40,80,30,10,96.887390,the\\n\"\n",
    "    \"4,1,1,2,2,0,20,100,100,10,-1,\\n\"\n",
    "    \"5,1,1,2,2,1,20,100,60,10,90.893219,<lazy>\\n\"\n",
    "    \"5,1,1,2,2,2,90,100,30,10,96.538940,dog\\n\"\n",
    "    \"1,2,0,0,0,0,0,0,300,400,-1,\\n\"\n",
    "    \"2,2,1,0,0,0,20,20,110,90,-1,\\n\"\n",
    "    \"3,2,1,1,0,0,20,20,180,30,-1,\\n\"\n",
    "    \"4,2,1,1,1,0,20,20,110,10,-1,\\n\"\n",
    "    \"5,2,1,1,1,1,20,20,30,10,96.063751,The\\n\"\n",
    "    \"5,2,1,1,1,2,60,20,50,10,95.965691,~groovy\\n\"\n",
    "    \"4,2,1,1,2,0,20,40,200,10,-1,\\n\"\n",
    "    \"5,2,1,1,2,1,20,40,70,10,95.835831,minute!\\n\"\n",
    "    \"5,2,1,1,2,2,100,40,30,10,94.899742,dog\\n\"\n",
    "    \"5,2,1,1,2,3,140,40,60,10,96.683357,bounds\\n\"\n",
    "    \"3,2,1,2,0,0,20,80,90,30,-1,\\n\"\n",
    "    \"4,2,1,2,1,0,20,80,80,10,-1,\\n\"\n",
    "    \"5,2,1,2,1,1,20,80,40,10,96.912064,UPON\\n\"\n",
    "    \"5,2,1,2,1,2,40,80,30,10,96.887390,the\\n\"\n",
    "    \"4,2,1,2,2,0,20,100,100,10,-1,\\n\"\n",
    "    \"5,2,1,2,2,1,20,100,60,10,90.893219,sleepy\\n\"\n",
    "    \"5,2,1,2,2,2,90,100,30,10,96.538940,fox\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow.csv import read_csv\n",
    "import io\n",
    "# from retrievall.basic import corpus_from_tesseract_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata fetchr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "from retrievall.core import AttrExpr, Chunks\n",
    "\n",
    "\n",
    "class AtomData(AttrExpr):\n",
    "    \"\"\"\n",
    "    Get atom metadata?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attrs: list):\n",
    "        self.attrs = attrs\n",
    "\n",
    "    def __call__(self, chunks: Chunks) -> pa.Array:\n",
    "        return (\n",
    "            pl.from_arrow(chunks.chunk_atoms)\n",
    "            .join(pl.from_arrow(chunks.corpus.atoms), left_on=\"atom\", right_on=\"id\")\n",
    "            .group_by(\"chunk\")\n",
    "            .agg(pl.col(self.attrs))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chunk: uint64\n",
       "tesseract_coord: large_list<item: struct<page_num: int64, block_num: int64, par_num: int64, line_num: int64, word_num: int64>>\n",
       "  child 0, item: struct<page_num: int64, block_num: int64, par_num: int64, line_num: int64, word_num: int64>\n",
       "      child 0, page_num: int64\n",
       "      child 1, block_num: int64\n",
       "      child 2, par_num: int64\n",
       "      child 3, line_num: int64\n",
       "      child 4, word_num: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from retrievall.ocr import corpus_from_tesseract_table\n",
    "\n",
    "corpus = corpus_from_tesseract_table(\n",
    "    read_csv(io.BytesIO(tesseract_csv.encode())), document_id=\"123\"\n",
    ")\n",
    "\n",
    "AtomData(\"tesseract_coord\")(corpus.chunk(\"page\")).to_arrow().schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>chunk</th><th>tesseract_coord</th></tr><tr><td>u64</td><td>list[struct[5]]</td></tr></thead><tbody><tr><td>14712110704280823429</td><td>[{1,1,1,1,1}, {1,1,1,1,2}, … {1,1,2,2,2}]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 2)\n",
       "┌──────────────────────┬─────────────────────────────────┐\n",
       "│ chunk                ┆ tesseract_coord                 │\n",
       "│ ---                  ┆ ---                             │\n",
       "│ u64                  ┆ list[struct[5]]                 │\n",
       "╞══════════════════════╪═════════════════════════════════╡\n",
       "│ 14712110704280823429 ┆ [{1,1,1,1,1}, {1,1,1,1,2}, … {… │\n",
       "└──────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AtomData(\"tesseract_coord\")(corpus.chunk(\"page\"))[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrievall.ocr import corpus_from_tesseract_table\n",
    "\n",
    "corpus = corpus_from_tesseract_table(\n",
    "    read_csv(io.BytesIO(tesseract_csv.encode())), document_id=\"123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "text: large_string\n",
       "----\n",
       "text: [[\"The (quick) [brown] fox jumps! Over the <lazy> dog\"]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from retrievall.filters import Threshold\n",
    "from retrievall.exprs import SimpleStringify\n",
    "\n",
    "(\n",
    "    corpus.chunk(\"page\")\n",
    "    .filter(Threshold(\"ordinal\", \"<=\", 1))\n",
    "    .select(text=SimpleStringify())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrievall.chunkers import FixedSizeChunk\n",
    "from retrievall.exprs import SimpleStringify\n",
    "from retrievall.sparsetext import Tfidf\n",
    "from retrievall.filters import TopK\n",
    "\n",
    "(\n",
    "    corpus.chunk(FixedSizeChunk(\"page\", 64, offset=-32))\n",
    "    .enrich(tfidf=Tfidf(SimpleStringify(), query=\"brown fox\"))\n",
    "    .filter(TopK(\"tfidf\", 3))\n",
    "    .select(text=SimpleStringify())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "Let's try to put this together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     corp.chunk(\n",
    "#         FixedSizeChunker(\"fixed\", size=3, offset=-1, constrain_to=\"line\")\n",
    "#     )  # Create some ephemeral chunks\n",
    "#     .enrich(tfidf=TfidfScorer(SimpleStringifier(), query=\"Inability to exercise\"))\n",
    "#     .filter(\"WHERE tfidf > 1.2\")\n",
    "#     .select(\n",
    "#         \"tfidf\",\n",
    "#         \"service_request_id\",\n",
    "#         OCRTokens(),  # ???\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we just do this with a `HAVING` SQL clause anyways? (see [this DuckDB page](https://duckdb.org/docs/sql/query_syntax/having#examples) for example):\n",
    "```sql\n",
    "SELECT\n",
    "    chunk_id,\n",
    "    SimpleStringifier(),\n",
    "    avg(income)\n",
    "FROM addresses\n",
    "GROUP BY city, street_name\n",
    "HAVING avg(income) > 2 * median(income);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a retrieval configuration\n",
    "If you're working with an automated job or an experimentation workflow, it may be handier to *configure* your retrieval process via a config file, rather than having to go in and make changes to your Python code by hand. Below is a helper function that can enable this configurability.\n",
    "\n",
    "(This is pretty hacked together, it might be smarter to try out something like [Pydantic](https://docs.pydantic.dev/latest/) if you want to do this seriously.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from retrievall import Corpus\n",
    "\n",
    "\n",
    "def config_retrieve(corpus: Corpus, cfg: dict) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Retrieve data from `corpus`, according to the retrieval process\n",
    "    defiend in `cfg`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus\n",
    "        `Corpus` to retrieve from.\n",
    "    cfg\n",
    "        Configuration dict. Requires 2 keys:\n",
    "        * `chunk`, has a value of either a string matching existing chunk\n",
    "          name, or a dict with a `name` key that declares the (qualified) name of the\n",
    "          `ChunkExpr` to use and an `args` key that contains a dict of arguments to pass\n",
    "          when instantiating the chunk expression.\n",
    "        * `retrieve`, which contains a list of dicts, each one with an `operation` key\n",
    "          (either `filter`, `enrich` or `select`) and an `args` dict. Arguments may be\n",
    "          strings or expressions, defined in the same way as the chunk expression above\n",
    "          (i.e. a dict with a `name` and an `args` key)\n",
    "    \"\"\"\n",
    "\n",
    "    def object_from_dict(d: dict):\n",
    "        \"\"\"\n",
    "        Instantiate an arbitrary object from a dict that contains a\n",
    "        `name` key (giving the class name) and an `args` key (providing\n",
    "        arguments to the instantation)\n",
    "        \"\"\"\n",
    "        module_name, class_name = d[\"name\"].rsplit(\".\", 1)\n",
    "        obj_class = getattr(importlib.import_module(module_name), class_name)\n",
    "\n",
    "        # Instantiate\n",
    "        return obj_class(**d[\"args\"])\n",
    "\n",
    "    chunkexpr = cfg[\"chunk\"]\n",
    "    # Instantiate chunk expression, if the `chunks` argument contains one\n",
    "    chunkexpr = chunkexpr if isinstance(chunkexpr, str) else object_from_dict(chunkexpr)\n",
    "\n",
    "    res = corpus.chunk(chunkexpr)\n",
    "\n",
    "    for op in cfg[\"retrieve\"]:\n",
    "        # If there's just one argument, instantiate it. Otherwise, loop\n",
    "        # through all passed arguments and instantiate them if possible.\n",
    "        if op[\"args\"].keys() == {\"args\", \"name\"}:\n",
    "            res = getattr(res, op[\"operation\"])(object_from_dict(op[\"args\"]))\n",
    "        else:\n",
    "            res = getattr(res, op[\"operation\"])(\n",
    "                **{\n",
    "                    k: v if isinstance(v, str) else object_from_dict(v)\n",
    "                    for k, v in op[\"args\"].items()\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Chunks' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[118], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43mconfig_retrieve\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrieve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moperation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrievall.filters.EqualTo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mordinal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moperation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mordinal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mordinal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrievall.filters.EqualTo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mordinal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     24\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;32m     25\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[1;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[111], line 48\u001b[0m, in \u001b[0;36mconfig_retrieve\u001b[0;34m(corpus, cfg)\u001b[0m\n",
      "\u001b[1;32m     46\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(res, op[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation\u001b[39m\u001b[38;5;124m\"\u001b[39m])(object_from_dict(op[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 48\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moperation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\n",
      "\u001b[1;32m     50\u001b[0m \u001b[43m                \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobject_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     51\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\n",
      "File \u001b[0;32m~/retrievall/src/core.py:257\u001b[0m, in \u001b[0;36mChunks.select\u001b[0;34m(self, *attrs, **exprs)\u001b[0m\n",
      "\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, expr \u001b[38;5;129;01min\u001b[39;00m exprs\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;32m    256\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks\u001b[38;5;241m.\u001b[39mselect(attrs) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m expr(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;32m--> 257\u001b[0m     selected \u001b[38;5;241m=\u001b[39m \u001b[43mselected\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m selected\n",
      "\n",
      "File \u001b[0;32m~/retrievall/.venv/lib/python3.10/site-packages/pyarrow/table.pxi:2400\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.append_column\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/retrievall/.venv/lib/python3.10/site-packages/pyarrow/table.pxi:5221\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.add_column\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/retrievall/.venv/lib/python3.10/site-packages/pyarrow/table.pxi:1478\u001b[0m, in \u001b[0;36mpyarrow.lib.chunked_array\u001b[0;34m()\u001b[0m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Chunks' object is not iterable"
     ]
    }
   ],
   "source": [
    "config_retrieve(\n",
    "    corpus,\n",
    "    {\n",
    "        \"chunk\": \"line\",\n",
    "        \"retrieve\": [\n",
    "            {\n",
    "                \"operation\": \"filter\",\n",
    "                \"args\": {\n",
    "                    \"name\": \"retrievall.filters.EqualTo\",\n",
    "                    \"args\": {\"attr\": \"ordinal\", \"values\": [1]},\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"operation\": \"select\",\n",
    "                \"args\": {\n",
    "                    \"ordinal\": \"ordinal\",\n",
    "                    \"text\": {\n",
    "                        \"name\": \"retrievall.filters.EqualTo\",\n",
    "                        \"args\": {\"attr\": \"ordinal\", \"values\": [1]},\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
